# -*- coding: utf-8 -*-
"""Copia de Trabajo_RRHH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QA20HjGHX0FweT0gIuS-1XUcfvq0xIqp

# Carga de paquetes
"""

import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# Importar librerias necesarias
!pip install ydata-profiling

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from ydata_profiling  import ProfileReport
from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# %matplotlib inline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

"""# Carga de las bases de datos"""

# importar datos
from google.colab import drive
drive.mount('/content/drive')

# cargar bases de datos
df_employee= pd.read_csv('/content/drive/MyDrive/Analitica3/employee_survey_data.csv',sep= None,engine="python")
df_general = pd.read_csv('/content/drive/MyDrive/Analitica3/general_data.csv',sep= None,engine="python")
df_manager = pd.read_csv('/content/drive/MyDrive/Analitica3/manager_survey_data.csv',sep= None,engine="python")
df_retirement = pd.read_csv('/content/drive/MyDrive/Analitica3/retirement_info.csv',sep= None,engine="python")
df_intime = pd.read_csv('/content/drive/MyDrive/Analitica3/in_time.csv',sep= None,engine="python")
df_outtime = pd.read_csv('/content/drive/MyDrive/Analitica3/out_time.csv',sep= None,engine="python")

"""# **Funciones**"""

#Función que realiza la imputación de valores nulos de una variable categorica y los reemplaza por la moda
def impute_categorical_variable(df, column_name):
    mode_value = df[column_name].mode()[0]
    df[column_name].fillna(mode_value, inplace=True)

# Función que realiza la imputación de valores nulos de una variable númerica y los reemplaza por la media
def impute_numeric_variable(df, column_name, strategy='median'):
    imp = SimpleImputer(strategy=strategy)
    column_data = df[column_name].values.reshape(-1, 1)
    imp.fit(column_data)
    df[column_name] = imp.transform(column_data)  # Ajustar imputer y transformar las variables con valores null

# función que convierte todas las columnas de un DataFrame en minisculas
def convertirmin(df):
    df.columns = map(str.lower,df.columns)

"""# **1. Preprocesamiento y exploración de los datos**"""

#convertir columnas de todos los df a miniscula
convertirmin(df_employee)
convertirmin(df_general)
convertirmin(df_manager)
convertirmin(df_retirement)
convertirmin(df_intime)
convertirmin(df_outtime)

df_employee.head()

df_general.head()

df_manager.head()

df_retirement.head()

df_intime.head()

df_outtime.head()

#unir la BD general con la BD de evaluacion de desempeño
df_temporal=pd.merge(df_general, df_manager, on="employeeid",how="inner")
df_temporal

#unimos la BD resultante con la BD de satisfacción con su empleo
df_consolidada=pd.merge(df_temporal, df_employee, on="employeeid",how="inner")
df_consolidada

#unimos la BD consolodida con la BD de los retiros
df_consol_retiros=pd.merge(df_consolidada, df_retirement, on="employeeid",how="outer")
df_consol_retiros

"""## 1.1 Exploración y tratamiento de la base consolidada (sin retiros)"""

df_consolidada.info()

df_consolidada.describe()

df_consolidada.head()

# Identificación de valores ausentes
df_consolidada.isnull().sum()

#Los datos nulos de la variable "numcompaniesworked" y "totalworkingyears" se reemplazarán por la media de los registros

df_consolidada1 = df_consolidada.copy()

impute_numeric_variable(df_consolidada1, "numcompaniesworked")

impute_numeric_variable(df_consolidada1, "totalworkingyears")

print(df_consolidada1.isnull().sum())

#Los datos nulos de la variables "environmentsatisfaction", "jobsatisfaction" y "worklifebalance", serán reemplazados por la moda de los registros

impute_categorical_variable(df_consolidada1, "environmentsatisfaction")

impute_categorical_variable(df_consolidada1, "jobsatisfaction")

impute_categorical_variable(df_consolidada1, "worklifebalance")


#Verificamos valores ausentes por columna
print(df_consolidada1.isnull().sum())

"""## **1.2 Exploración y tratamiento de la base consolidada (con retiros)**



"""

#unimos la BD consolodida con la BD de los retiros
df_consol_retiros=pd.merge(df_consolidada1, df_retirement, on="employeeid",how="outer")
df_consol_retiros

df_consol_retiros.info()

df_consol_retiros.describe()

df_consol_retiros.head()

# Identificación de valores ausentes
df_consol_retiros.isnull().sum()

df_consol_retiros1 = df_consol_retiros.copy() #crear una copia del DF para tratar los datos nulos
# Llamada a la función
impute_categorical_variable(df_consol_retiros1, "resignationreason")

# Identificación de valores ausentes
df_consol_retiros1.isnull().sum()

"""# **2. Analisis exploratorio**"""

# --- Reporte exploratorio del dataset ---
ProfileReport(df_consolidada1, title='Reporte exploratorio del dataset consolidado sin retiros', minimal=True,
              progress_bar=False, interactions=None, explorative=True, dark_mode=True,
              notebook={'iframe':{'height': '600px'}}, html={'style':{'primary_color': '#FFCC00'}},
              missing_diagrams={'heatmap': False, 'dendrogram': False}).to_notebook_iframe()

##Histograma de las variables department, gender y jobsatisfaction
plt.figure(figsize=(20, 6))
plt.subplot(1,3,1)
plt.hist(df_consolidada1.department,5)
plt.xlabel('department')
plt.subplot(1,3,2)
plt.hist(df_consolidada1.gender,3)
plt.xlabel('gender')
plt.subplot(1,3,3)
plt.hist(df_consolidada1.jobsatisfaction,7)
plt.xlabel('jobsatisfaction')
plt.show()

"""***CONCLUSIÓN:***

1.   Histograma *Department*: Se observa que el departamento con mayor número de empleados es el de *Ivestigación y desarrollo* seguido de *Ventas* y en úultimo lugar con menos de 200 empleados el departamento de *Recursos humanos*
2.   Histograma *Gender2*: Se puede observar que la empresa tiene más empleados hombres que mujeres.
3. Histograma *Jobsatisfaction*: Se observa que una buena parte de los empleados se encuentra medianamente satisfechos con su trabajo o satifechos, mientras que otra parte corresposdiente casi a la mitad de los empleados no se encuentran satisfechos con su trabajo.

"""

##Revisemos el comportamiento de la variable attrition, pues esta será nuestra variable de respuesta para la regresión
#Histograma de variables para identificar su comportamiento
df_consolidada1.hist(bins=50, figsize=(20,15))
plt.show()

# --- Correlation Map (Heatmap) ---
mask = np.triu(np.ones_like(df_consolidada1.corr(), dtype=bool))
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(df_consolidada1.corr(), center = 1, mask = mask, cbar = True, annot = True, ax = ax, annot_kws={"size":8})
plt.show()

##Correlaciones base sin retiros
x = df_consolidada1.corr().unstack().sort_values()
x[x<1].tail(10)

##Correlaciones base con retiros
x = df_consol_retiros1.corr().unstack().sort_values()
x[x<1].tail(10)

#Determinar el número de empleados por departamento
emp_dep= df_consolidada1.groupby(["department"])[["employeecount"]].count().sort_values('employeecount', ascending = False).reset_index()
emp_dep

#grafico para visualizar mejor el porcentaje de trabajadores por departamento
fig = px.pie(emp_dep, values = 'employeecount', names = 'department', title = '<b>% Porcentaje de empleados por departamento<b>')

fig.update_layout(
    template = 'simple_white',
    legend_title = 'department',
    title_x = 0.5)
fig.show()

## Que tanto estuvieron involucrados las personas retiradas en proyectos segun sus estudios
pd.crosstab(index= df_consol_retiros['educationfield'],
            columns=df_consol_retiros['jobinvolvement']).plot(kind='bar')

## Cantidad de personas segun su departamento y qué satisfechos están.
pd.crosstab(index=df_consol_retiros['department'],
            columns=df_consol_retiros['jobsatisfaction']).plot(kind='bar')

##Satisfacción en el trabajo respecto a los años que lleva en la empresa

fig = px.histogram(df_consolidada1, x="yearsatcompany", color="jobsatisfaction")
fig.show()

"""**CONCLUSIÓN**

En general, además de ver que la mayoria de empleados lleva entre 0 y 20 años en la empresa, se observa que gran proporción de empelados están medianamente satisfechos o satisfechos con su empleo,
Sin embargo también hay un cantidad considerable de personas insatisfechas o medianamente insatisfechas
"""

## De los retirados de los diferentes departamentos, se realizaron de 2 a 3 capacitaciones
plt.figure(figsize = (14,9))
sns.countplot(data=df_consol_retiros1,x="department",hue="trainingtimeslastyear")

df_consol_retiros1['retirementdate'] = pd.to_datetime(df_consol_retiros1['retirementdate']) # Convertir la fecha en el formato correcto
df_consol_retiros1["retirementdate"] = df_consol_retiros1["retirementdate"].dt.month
retfecha= df_consol_retiros1.groupby(["retirementdate"])[["employeecount"]].count().reset_index()
fig = px.line(retfecha, x='retirementdate', y =['employeecount'], title = '<b>Evolución de retiros de los empleados<b>',
              color_discrete_sequence=px.colors.qualitative.G10)
fig.update_layout(
    template = 'simple_white',
    title_x = 0.5,
    legend_title = 'Despidos:',
    xaxis_title = '<b>Fecha<b>',
    yaxis_title = '<b>Cantidad de casos<b>',
)

fig.show()

#Tabla con el número de retiros por mes
retfecha

##Verificar motivo de retiro
df_consol_retiros1["resignationreason"].value_counts()
fig = px.bar(df_consol_retiros1, x=df_consol_retiros1.index, y="resignationreason", title="Clasificación de Razones de Renuncia")
fig.update_xaxes(title="Cantidad")
fig.update_yaxes(title="Razones de Renuncia")

fig.show()

#Número de empleados por tipo de renuncia
df_consol_retiros1["resignationreason"].value_counts()

"""# 3. Preparación de los datos"""

df_consol_retiros1["retirementtype"].value_counts()

##Dejamos por fuera a los que despidieron porque nuestro modelo se basa en los RETIRADOS
df_consol_retiros1["salaryhike"] = df_consol_retiros1['percentsalaryhike'].apply(lambda x: x/100)
df_consol_retiros1["renuncia"] = df_consol_retiros1["retirementtype"].apply(lambda x: 0 if x != 'Resignation' else 1)

## Se eliminan variables que creemos que no aportan al modelo
df_consol_retiros1.drop(columns=['employeecount','standardhours','over18','percentsalaryhike','retirementdate','retirementtype','resignationreason'], axis= 1,inplace=True)

plt.figure(figsize =(15,15))
sns.heatmap(df_consol_retiros1.corr(),annot= True)

##Correlaciones
x = df_consol_retiros1.corr().unstack().sort_values()
x[x<1].tail(10)

##A raíz de la selección de variables, se convierten en dummies aquellas que son categoricas.
dummies= pd.get_dummies(df_consol_retiros1, columns=["businesstravel","department","educationfield","gender","jobrole","maritalstatus","attrition"])
dummies

##División de los datos
X= dummies.drop(["renuncia"],axis = 1)
y = dummies.renuncia

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import OneHotEncoder
escalador = MinMaxScaler()
dReescalados = escalador.fit_transform(X)
#set_printoptions(precision=2)
print(dReescalados)

escalador = StandardScaler().fit(X)
dEstandarizados = escalador.transform(X)
dEstandarizados

escalador = Normalizer().fit(X)
dEstandarizados = escalador.transform(X)
dEstandarizados

"""#** Selección de modelos**"""

#Crear un modelo de selección
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from numpy import set_printoptions

est_prueba = SelectKBest(score_func=f_classif, k='all')
est_ajustado = est_prueba.fit(X,y)

#Muestro el desempeño de los features basado en el valor F
set_printoptions(precision=3, suppress= True)
est_ajustado.scores_

#Estas son las variables que se utilizarán para el modelo
df_modelo = df_consol_retiros1[["age","employeeid","stockoptionlevel","trainingtimeslastyear","yearssincelastpromotion",
                       "yearswithcurrmanager","environmentsatisfaction","salaryhike","businesstravel",
                       "department","attrition","maritalstatus"]]

"""## **Random Forest**"""

# Para poder modelar
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

#para visualización
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

test_size=0.33
seed=6
X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X, y, test_size=test_size,random_state=seed)

rf = RandomForestClassifier(n_estimators=100, random_state=0)
modelo_forest= rf.fit(X_train1, Y_train1)

y_pred1 = rf.predict(X_test1) #predicción

accuracy = accuracy_score(Y_test1, y_pred1)
print("Accuracy:", accuracy)

matrix = confusion_matrix(Y_test1, y_pred1)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()

reporte = classification_report(Y_test1,y_pred1) #ramdom_forest
print(reporte)

param_dist = {'n_estimators': randint(50,500), #esto es para conocer los mejores hiperparametros
              'max_depth': randint(1,20)}

# Creaar un random forest classifier
rf = RandomForestClassifier()

# Usar un random search para encontrar los mejores hiperparametros
rand_search = RandomizedSearchCV(rf,
                                 param_distributions = param_dist,
                                 n_iter=5,
                                 cv=5)

# Ajustar el random search object a los datos
rand_search.fit(X_train1, Y_train1)

# Crear una variable para el mejor modelo
best_rf = rand_search.best_estimator_

# imprimir los mejores hiperparametros
print('Best hyperparameters:',  rand_search.best_params_)

# Crear una serie que contenga la importancia de las características del modelo y nombres de características de los datos de entrenamiento
feature_importances = pd.Series(best_rf.feature_importances_, index=X_train1.columns).sort_values(ascending=False)

# Trazar un gráfico de barras simple
plt.figure(figsize= (12,9))
feature_importances.plot.bar();

"""## **Neuronal Network**"""

from sklearn.datasets import make_classification
from sklearn.neural_network import MLPClassifier
X, y = make_classification(n_samples=4000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
regr = MLPClassifier(random_state=1, max_iter=500).fit(X_train, y_train)
pred = regr.predict(X_test[:2])
regr.score(X_test, y_test)

#Evaluar con K Fold cross-validation
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
kfld = KFold(n_splits=10, random_state=6, shuffle=True)
modelo = regr
res = cross_val_score(modelo, X, y, cv=kfld)
mean= res.mean()*100
print(res)
print(mean)

#Evaluar con Area under ROC
kfold = KFold(n_splits=10, random_state=6, shuffle=True)
modelo = regr
score='roc_auc'
resultado = cross_val_score(modelo, X, y, cv=kfold, scoring=score)
cl2= resultado.mean()*100
cl2

predictionsRN = modelo.predict(X_test)
print(accuracy_score(y_test, predictionsRN))

print(classification_report(y_test, predictionsRN))

"""## **Regresión logística**"""

from sklearn import linear_model
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
model = linear_model.LogisticRegression()
model.fit(X,y)
model.score(X,y)

validation_size = 0.20
seed = 7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)
name='Logistic Regression'
kfold = model_selection.KFold(n_splits=10,shuffle = False, random_state=None)
cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
print(msg)

predictionsRL = model.predict(X_validation)
print(accuracy_score(Y_validation, predictionsRL))

matrix = confusion_matrix(Y_validation, predictionsRL)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()

print(classification_report(Y_validation, predictionsRL))

"""## **SVC Linear**"""

dum_modelo = pd.get_dummies(df_modelo)
standard_scalar = StandardScaler()
data_scaled = standard_scalar.fit_transform(dum_modelo)
data = pd.DataFrame(data_scaled, columns=dum_modelo.columns)

from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

x, y = make_classification(n_samples=2100, n_features=10,
                           n_classes=2,
                           n_clusters_per_class=1)

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20)

lsvc = LinearSVC(verbose=0)
print(lsvc)

LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
          verbose=0)

lsvc.fit(xtrain, ytrain)
score = lsvc.score(xtrain, ytrain)
print("Score: ", score)

cv_scores = cross_val_score(lsvc, xtrain, ytrain, cv=10)
print("CV average score: %.2f" % cv_scores.mean())

ypred = lsvc.predict(xtest)

cm = confusion_matrix(ytest, ypred)
print(cm)

cr = classification_report(ytest, ypred)
print(cr)

from sklearn.metrics import ConfusionMatrixDisplay

lsvc.fit(xtrain, ytrain)
predicted = lsvc.predict(xtest)
matrix = confusion_matrix(ytest, predicted)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()