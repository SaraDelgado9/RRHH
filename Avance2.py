# -*- coding: utf-8 -*-
"""Avance2_RRHH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QA20HjGHX0FweT0gIuS-1XUcfvq0xIqp

# Carga de paquetes
"""

import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# Importar librerias necesarias
!pip install ydata-profiling

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from ydata_profiling  import ProfileReport
from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# %matplotlib inline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

"""# Carga de las bases de datos"""

# importar datos
from google.colab import drive
drive.mount('/content/drive')

# cargar bases de datos
df_employee= pd.read_csv('/content/drive/MyDrive/Analitica3/employee_survey_data.csv',sep= None,engine="python")
df_general = pd.read_csv('/content/drive/MyDrive/Analitica3/general_data.csv',sep= None,engine="python")
df_manager = pd.read_csv('/content/drive/MyDrive/Analitica3/manager_survey_data.csv',sep= None,engine="python")
df_retirement = pd.read_csv('/content/drive/MyDrive/Analitica3/retirement_info.csv',sep= None,engine="python")
df_intime = pd.read_csv('/content/drive/MyDrive/Analitica3/in_time.csv',sep= None,engine="python")
df_outtime = pd.read_csv('/content/drive/MyDrive/Analitica3/out_time.csv',sep= None,engine="python")

"""# **Funciones**"""

##Función para imputar valores nulos de las variables categoricas y reemplazarlos por la moda
def impute_categorical_variable(df, column_name):
    mode_value = df[column_name].mode()[0]
    df[column_name].fillna(mode_value, inplace=True)

##Función para imputar valores nulos de las variables númericas y reemplazarlos por la media
def impute_numeric_variable(df, column_name, strategy='median'):
    imp = SimpleImputer(strategy=strategy)
    column_data = df[column_name].values.reshape(-1, 1)
    imp.fit(column_data)
    df[column_name] = imp.transform(column_data)  # Ajustar imputer y transformar las variables con valores null

##Función para conviertir todas las columnas de un DF en minisculas
def convert_min(df):
    df.columns = map(str.lower,df.columns)

"""# **1. Preprocesamiento y exploración de los datos**"""

#convertir columnas de todos los df a miniscula
convert_min(df_employee)
convert_min(df_general)
convert_min(df_manager)
convert_min(df_retirement)
convert_min(df_intime)
convert_min(df_outtime)

df_employee.head()

df_general.head()

df_manager.head()

df_retirement.head()

df_intime.head()

df_outtime.head()

##Union bases de datos

#unir la BD general con la BD de evaluacion de desempeño
df_temporal=pd.merge(df_general, df_employee, on="employeeid",how="inner")
#unir la BD resultante con la BD de satisfacción con su empleo
df_consolidada=pd.merge(df_temporal, df_manager, on="employeeid",how="inner")
#unimos la BD consolodida con la BD de los retiros
df_consol_retiros=pd.merge(df_consolidada, df_retirement, on="employeeid",how="outer")

df_consol_retiros

"""## 1.1 Exploración y tratamiento de la base consolidada (sin retiros)"""

df_consolidada.info()

df_consolidada.head()

df_consolidada.describe()

# Identificación de valores ausentes
df_consolidada.isnull().sum()

#Los datos nulos de la variable "numcompaniesworked" y "totalworkingyears" se reemplazarán por la media de los registros

df_consolidada1 = df_consolidada.copy()

impute_numeric_variable(df_consolidada1, "numcompaniesworked")

impute_numeric_variable(df_consolidada1, "totalworkingyears")

print(df_consolidada1.isnull().sum())

#Los datos nulos de la variables "environmentsatisfaction", "jobsatisfaction" y "worklifebalance", serán reemplazados por la moda de los registros

impute_categorical_variable(df_consolidada1, "environmentsatisfaction")

impute_categorical_variable(df_consolidada1, "jobsatisfaction")

impute_categorical_variable(df_consolidada1, "worklifebalance")


#Verificamos valores ausentes por columna
print(df_consolidada1.isnull().sum())

"""## **1.2 Exploración y tratamiento de la base consolidada (con retiros)**



"""

#unimos la BD consolodida1 con la BD de los retiros y la reemplazamos
df_consol_retiros=pd.merge(df_consolidada1, df_retirement, on="employeeid",how="outer")
df_consol_retiros

df_consol_retiros1 = df_consol_retiros.copy() #crear una copia del DF para tratar los datos nulos
# Llamar la función
impute_categorical_variable(df_consol_retiros1, "resignationreason")

"""# **2. Analisis exploratorio**"""

# --- Reporte exploratorio del dataset ---
ProfileReport(df_consolidada1, title='Reporte exploratorio del dataset consolidado sin retiros', minimal=True,
              progress_bar=False, interactions=None, explorative=True, dark_mode=True,
              notebook={'iframe':{'height': '600px'}}, html={'style':{'primary_color': '#FFCC00'}},
              missing_diagrams={'heatmap': False, 'dendrogram': False}).to_notebook_iframe()

##Histograma de las variables department, gender y jobsatisfaction
plt.figure(figsize=(20, 6))
plt.subplot(1,3,1)
plt.hist(df_consolidada1.department,5)
plt.xlabel('department')
plt.subplot(1,3,2)
plt.hist(df_consolidada1.gender,3)
plt.xlabel('gender')
plt.subplot(1,3,3)
plt.hist(df_consolidada1.jobsatisfaction,7)
plt.xlabel('jobsatisfaction')
plt.show()

##Revisemos el comportamiento de la variable attrition
#Histograma de variables para identificar su comportamiento
df_consolidada1.hist(bins=50, figsize=(20,15))
plt.show()

# --- Correlation Map (Heatmap) ---
mask = np.triu(np.ones_like(df_consolidada1.corr(), dtype=bool))
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(df_consolidada1.corr(), center = 1, mask = mask, cbar = True, annot = True, ax = ax, annot_kws={"size":8})
plt.show()

#Determinar el número de empleados por departamento
emp_dep= df_consolidada1.groupby(["department"])[["employeecount"]].count().sort_values('employeecount', ascending = False).reset_index()
emp_dep

#grafico para visualizar mejor el porcentaje de trabajadores por departamento
fig = px.pie(emp_dep, values = 'employeecount', names = 'department', title = '<b>% Porcentaje de empleados por departamento<b>')

fig.update_layout(
    template = 'simple_white',
    legend_title = 'department',
    title_x = 0.5)
fig.show()

## Que tanto estuvieron involucrados las personas retiradas en proyectos segun sus estudios
pd.crosstab(index= df_consol_retiros['educationfield'],
            columns=df_consol_retiros['jobinvolvement']).plot(kind='bar')

## Cantidad de personas segun su departamento y qué tan satisfechos estan.
pd.crosstab(index=df_consol_retiros['department'],
            columns=df_consol_retiros['jobsatisfaction']).plot(kind='bar')

##Satisfacción en el trabajo respecto a los años que lleva en la empresa

fig = px.histogram(df_consolidada1, x="yearsatcompany", color="jobsatisfaction")
fig.show()

## De los retirados de los diferentes departamentos, se realizaron de 2 a 3 capacitaciones
plt.figure(figsize = (14,9))
sns.countplot(data=df_consol_retiros1,x="department",hue="trainingtimeslastyear")

##Verificar motivo de retiro
df_consol_retiros1["resignationreason"].value_counts()
fig = px.bar(df_consol_retiros1, x=df_consol_retiros1.index, y="resignationreason", title="Clasificación de Razones de Renuncia")
fig.update_xaxes(title="Cantidad")
fig.update_yaxes(title="Razones de Renuncia")

fig.show()

#Número de empleados por tipo de renuncia
df_consol_retiros1["resignationreason"].value_counts()

##histograma relacion salario vs genero
fig = px.histogram(df_consolidada1, x="monthlyincome", color= "gender")
fig.show()

##boxplot salario vs motivo retiro y nivel de trabajo

fig= px.box(df_consol_retiros1, x="resignationreason", y="monthlyincome", color= "joblevel")
fig.show()

df_consol_retiros['retirementdate'] = pd.to_datetime(df_consol_retiros['retirementdate']) # Convertir la fecha en el formato
df_consol_retiros["retirementdate"] = df_consol_retiros["retirementdate"].dt.month
ret_mes= df_consol_retiros.groupby(["retirementdate"])[["employeecount"]].count().reset_index()
fig = px.line(ret_mes, x='retirementdate', y =['employeecount'], title = '<b>Evolución de retiros de los empleados<b>',
              color_discrete_sequence=px.colors.qualitative.G10)
fig.update_layout(template = 'simple_white',
                  title_x = 0.5,
                  legend_title = 'Despidos:',
                  xaxis_title = '<b>Fecha<b>',
                  yaxis_title = '<b>Cantidad de casos<b>',)
fig.show()

ret_mes

"""# 3. Preparación de los datos"""

df_consol_retiros1["retirementtype"].value_counts()

##RETIRADOS
df_consol_retiros1["salaryhike"] = df_consol_retiros1['percentsalaryhike'].apply(lambda x: x/100)
df_consol_retiros1["renuncia"] = df_consol_retiros1["retirementtype"].apply(lambda x: 0 if x != 'Resignation' else 1)

## Se eliminan variables que creemos que no aportan al modelo
df_consol_retiros1.drop(columns=['employeecount','standardhours','over18','percentsalaryhike','retirementdate','retirementtype','resignationreason'], axis= 1,inplace=True)

plt.figure(figsize =(18,18))
sns.heatmap(df_consol_retiros1.corr(),annot= True)

##Correlaciones
x = df_consol_retiros1.corr().unstack().sort_values()
x[x<1].tail(20)

##Conversion a dummies de las categoricas
dummies= pd.get_dummies(df_consol_retiros1, columns=["businesstravel","department","educationfield","gender","jobrole","maritalstatus","attrition"])
dummies

##División de los datos
X= dummies.drop(["renuncia"],axis = 1)
y = dummies.renuncia

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import OneHotEncoder

escalador = MinMaxScaler()
Reescalados = escalador.fit_transform(X)
print(Reescalados)

#escalamos
escalador = StandardScaler().fit(X)
Estandarizados = escalador.transform(X)
Estandarizados

#estandarizamos
escalador = Normalizer().fit(X)
Estandarizados = escalador.transform(X)
Estandarizados

"""#** Selección de modelos**"""

#Crear un modelo de selección
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from numpy import set_printoptions

#SelectKBest
sel_prueba = SelectKBest(score_func=f_classif, k='all')
sel_ajustado = sel_prueba.fit(X,y)

#Muestro el desempeño de los features basado en el valor F
set_printoptions(precision=3, suppress= True)
sel_ajustado.scores_

#Con base en el Select KBest, estas son las variables que se utilizarán para el modelo

df_modelo = df_consol_retiros1[["age","employeeid","stockoptionlevel","trainingtimeslastyear","yearssincelastpromotion",
                       "yearswithcurrmanager","environmentsatisfaction","salaryhike","businesstravel",
                       "department","attrition","maritalstatus"]]

"""## **Random Forest**"""

# Otras librerias
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

# datos de entrenamiento
test_size=0.33
seed=6
X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X, y, test_size=test_size,random_state=seed)

RF = RandomForestClassifier(n_estimators=100, random_state=0)
modelo_forest= RF.fit(X_train1, Y_train1)
#predicción
y_pred1 = RF.predict(X_test1)

accuracy = accuracy_score(Y_test1, y_pred1)
print("Accuracy:", accuracy)

#Matrix de confusion

matrix = confusion_matrix(Y_test1, y_pred1)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()

summary_RF = classification_report(Y_test1,y_pred1) #ramdom_forest
print(summary_RF)

#Estudio de hiperparametros
param_dist = {'n_estimators': randint(50,500),
              'max_depth': randint(1,20)}

# RF classifier
rf = RandomForestClassifier()

# Usar un random search para encontrar los mejores hiperparametros
rand_search = RandomizedSearchCV(rf,
                                 param_distributions = param_dist,
                                 n_iter=5,
                                 cv=5)

# Ajustar el random search object a los datos
rand_search.fit(X_train1, Y_train1)

# Crear una variable para el mejor modelo
best_rf = rand_search.best_estimator_

# mejores hiperparametros
print(rand_search.best_params_)

# Crear una serie que contenga la importancia de las características del modelo y nombres de características de los datos de entrenamiento
feature_importances = pd.Series(best_rf.feature_importances_, index=X_train1.columns).sort_values(ascending=False)

# Trazar un gráfico de barras simple
plt.figure(figsize= (12,9))
feature_importances.plot.bar();

"""## **Neuronal Network**"""

from sklearn.datasets import make_classification
from sklearn.neural_network import MLPClassifier

X, y = make_classification(n_samples=4000, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
regr = MLPClassifier(random_state=1, max_iter=500).fit(X_train, y_train)
pred = regr.predict(X_test[:2])
regr.score(X_test, y_test)

#EVALUCAIONES
# K Fold cross-validation
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
kfld = KFold(n_splits=10, random_state=6, shuffle=True)
modelo = regr
res = cross_val_score(modelo, X, y, cv=kfld)
mean= res.mean()*100
print(res)
print(mean)

# Area under ROC
kfold = KFold(n_splits=10, random_state=6, shuffle=True)
modelo = regr
score='roc_auc'
resultado = cross_val_score(modelo, X, y, cv=kfold, scoring=score)
cl2= resultado.mean()*100
cl2

#Prediccion RN
predictionsRN = modelo.predict(X_test)
print(accuracy_score(y_test, predictionsRN))

#Summary
print(classification_report(y_test, predictionsRN))

"""## **Regresión logística**"""

from sklearn import linear_model
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

model_reg = linear_model.LogisticRegression()
model_reg.fit(X,y)
model_reg.score(X,y)

validation_size = 0.20
seed = 7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)
kfold = model_selection.KFold(n_splits=10,shuffle = False, random_state=None)
cv_results = model_selection.cross_val_score(model_reg, X_train, Y_train, cv=kfold, scoring='accuracy')

print(cv_results.mean())
print(cv_results.std())

#Prediccion
predictionsRL = model_reg.predict(X_validation)
print(accuracy_score(Y_validation, predictionsRL))

matrix = confusion_matrix(Y_validation, predictionsRL)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()

#Summary
print(classification_report(Y_validation, predictionsRL))

"""## **SVC Linear**"""

#librerias
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

#se pasan las variables a dummies
modelo_dummies = pd.get_dummies(df_modelo)
#se escala
standard_scalar = StandardScaler()
#se estandariza
data_scaled = standard_scalar.fit_transform(modelo_dummies)
df = pd.DataFrame(data_scaled, columns=modelo_dummies.columns)

x, y = make_classification(n_samples=2100, n_features=10,
                           n_classes=2,
                           n_clusters_per_class=1)

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20)

lsvc = LinearSVC(verbose=0)
print(lsvc)

LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
          verbose=0)

#Score
lsvc.fit(xtrain, ytrain)
score = lsvc.score(xtrain, ytrain)
print(score)

#Average score
cv_scores = cross_val_score(lsvc, xtrain, ytrain, cv=10)
print(cv_scores.mean())

#Prediccion
ypred = lsvc.predict(xtest)

#Matrix de confusion
matrix = confusion_matrix(ytest, ypred)
display = ConfusionMatrixDisplay(confusion_matrix = matrix)
display.plot()
plt.show()

#Summary
summary_CV = classification_report(ytest, ypred)
print(summary_CV)